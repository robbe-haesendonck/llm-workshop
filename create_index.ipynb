{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load env variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingesting data and creating an index\n",
    "The first part in any RAG adventure is creating an index.\n",
    "\n",
    "To do this we'll have to load our data which will give us a list of `Document`. After this we still need to transform each Document into a Node.\n",
    "\n",
    "Documents and nodes not only contain text but also any usefull metadata (tags, where did the data come from, ...) which can all be set by us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from Markdown files\n",
    "The first step is loading data from Markdown files. If you need other file formats or locations you can take a look at the plethora of readers on llamahub.ai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.readers.file import MarkdownReader\n",
    "\n",
    "# Markdown parser will remove hyperlinks and images by default\n",
    "markdown_parser = MarkdownReader()\n",
    "file_extractors = {\n",
    "    \".md\": markdown_parser,\n",
    "}\n",
    "\n",
    "documents = SimpleDirectoryReader(\n",
    "    './data',\n",
    "    file_extractor=file_extractors\n",
    ").load_data()\n",
    "\n",
    "if len(documents) >= 1:\n",
    "    print(documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Documents to an index\n",
    "The next step, now that we've got a reference to our Documents is creating a searchable index from these documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.indices.managed.colbert import ColbertIndex\n",
    "\n",
    "index = ColbertIndex(\n",
    "    nodes=documents,\n",
    "    index_name='factoids'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing the index\n",
    "To store the index, you can simply call the `.persist` method on the index. This stores it to the local fs allowing it to be loaded in by (for example) a different process. (Run `python3 interactive.py` to see it in action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store index\n",
    "index.persist(persist_dir=\"./index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying example\n",
    "After you've created the index, you can start querying it.\n",
    "To do this, you first need to cast the index to a query engine using `index.as_query_engine()`.\n",
    "\n",
    "In this case, Groq is used for the inference/generation of the answers, but this can be exchanged for any of the supported LLM's by LlamaIndex.\n",
    "\n",
    "Another great feature is that results can be inspected, and relevant metadata can thus be extracted from it (such as the source document, file path, ...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import getenv\n",
    "from llama_index.llms.groq import Groq\n",
    "\n",
    "query = \"How are alerts configured in Prometheus? Please provide an example as well.\"\n",
    "\n",
    "llm = Groq(\n",
    "    model=\"llama3-8b-8192\",\n",
    "    api_key=getenv(\"GROQ_API_KEY\")\n",
    ")\n",
    "\n",
    "query_engine = index.as_query_engine(\n",
    "    llm=llm,\n",
    "    top_k=5\n",
    ")\n",
    "\n",
    "result = query_engine.query(\n",
    "    query\n",
    ")\n",
    "\n",
    "print(\"Answer: {}\".format(result))\n",
    "print(\"Sources: \\n{}\".format(result.get_formatted_sources(length=500)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatting instead of querying\n",
    "If you want to be able to truly chat with the documents/index you need to cast it to a chat engine.\n",
    "This keeps the history and other relevant information allowing for multi-turn chatting and follow-up questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.chat_engine.types import ChatMode\n",
    "\n",
    "chat_engine = index.as_chat_engine(\n",
    "    llm=llm,\n",
    "    chat_mode=ChatMode.CONDENSE_PLUS_CONTEXT,\n",
    ")\n",
    "\n",
    "chat_engine.reset()\n",
    "\n",
    "# Every chat call is added to the history\n",
    "\n",
    "response = chat_engine.chat(\n",
    "    \"What service discovery mechanisms are there in Prometheus?\"\n",
    ")\n",
    "\n",
    "print(\"Chat response: {}\".format(response))\n",
    "\n",
    "response = chat_engine.chat(\n",
    "    \"Can you show an example of how to do this for all of them?\"\n",
    ")\n",
    "\n",
    "print(\"Chat response: {}\".format(response))\n",
    "\n",
    "\n",
    "response = chat_engine.chat(\n",
    "    \"Please create a config that scrapes from a node exporter which is discovered by using HTTP service discovery.\"\n",
    ")\n",
    "print(\"Chat response: {}\".format(response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
